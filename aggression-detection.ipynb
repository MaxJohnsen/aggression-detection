{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggression Detection\n",
    "\n",
    "https://sites.google.com/view/trac1/shared-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from google.cloud import translate\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_SIZE = 20000\n",
    "MAX_SEQ_LENGTH = 150\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "augmentation_languages = ['ger','fre','ru']\n",
    "\n",
    "training_data = 'train/agr_en_train.csv'\n",
    "validation_data = 'train/agr_en_dev.csv'\n",
    "test_data = 'test/agr_en_fb_gold.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "categories = {'NAG': [1,0,0], 'CAG': [0,1,0], 'OAG': [0,0,1]}\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument_and_save(row, language, filename):\n",
    "    sentence = str(row[1].strip('\\\"').strip('\\''))\n",
    "\n",
    "    # Translate sentence to target language\n",
    "    translation = translate_client.translate(\n",
    "                sentence,\n",
    "                source_language='eng',\n",
    "                target_language=language)['translatedText']\n",
    "\n",
    "    # Translate sentence from target language to english\n",
    "    augmentation = translate_client.translate(\n",
    "                translation,\n",
    "                source_language=language,\n",
    "                target_language='eng')['translatedText']\n",
    "\n",
    "    # Save to disk\n",
    "    with open('augmented/'+filename+\"_\"+language+\".csv\", 'a') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow([row[0], augmentation, row[2]])\n",
    "\n",
    "    return augmentation\n",
    "\n",
    "def preprocess_data(sentence):\n",
    "    # Tokenize sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    # Lemmatizations\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # Remove stop-words\n",
    "    tokens = [word for word in tokens if word not in STOPWORDS]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_client = translate.Client()\n",
    "\n",
    "with open(training_data) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:            \n",
    "        filename = training_data.split(\"/\")[-1].replace('.csv', '')\n",
    "\n",
    "        for lang in augmentation_languages:\n",
    "            attemps = 0\n",
    "            while(attemps<3):\n",
    "                try:\n",
    "                    augument_and_save(row, lang, filename)\n",
    "                    break\n",
    "                except:\n",
    "                    attemps += 1\n",
    "                    print(\"Google Translate API req failed, sleeping for 1 min before trying again..\")\n",
    "                    time.sleep(60)\n",
    "                    \n",
    "            if attemps == 3:\n",
    "                raise RuntimeError(\"Google Translate API request failed 3 times in a row.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "val_X = []\n",
    "val_y = []\n",
    "\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "training_paths = [training_data]\n",
    "\n",
    "counter = {'NAG':0,'CAG':0,'OAG':0}\n",
    "\n",
    "\n",
    "# Add paths to the augumented data\n",
    "for language in augmentation_languages:\n",
    "    filename = training_data.split(\"/\")[-1].replace('.csv', '')\n",
    "    training_paths.append('augmented/'+filename+'_'+language+'.csv')\n",
    "\n",
    "# Import training data\n",
    "for path in training_paths:\n",
    "    with open(path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            sentence = str(row[1].strip('\\\"').strip('\\''))\n",
    "            counter[str(row[2])] += 1\n",
    "            train_X.append(preprocess_data(sentence))\n",
    "            train_y.append(categories[str(row[2])])\n",
    "\n",
    "# Import validation data\n",
    "with open(validation_data) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            sentence = str(row[1].strip('\\\"').strip('\\''))\n",
    "\n",
    "            val_X.append(preprocess_data(sentence))\n",
    "            val_y.append(categories[str(row[2])])\n",
    "            \n",
    "# Import test data\n",
    "with open(test_data) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            sentence = str(row[1].strip('\\\"').strip('\\''))\n",
    "\n",
    "            test_X.append(preprocess_data(sentence))\n",
    "            test_y.append(categories[str(row[2])])\n",
    "            \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_vec = pad_sequences(tokenizer.texts_to_sequences(train_X), maxlen=MAX_SEQ_LENGTH)\n",
    "val_X_vec = pad_sequences(tokenizer.texts_to_sequences(val_X), maxlen=MAX_SEQ_LENGTH)\n",
    "test_X_vec = pad_sequences(tokenizer.texts_to_sequences(test_X), maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "print('train_X_vec shape:', train_X_vec.shape)\n",
    "print('val_X_vec shape:', val_X_vec.shape)\n",
    "print('test_X_vec shape:', test_X_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_vec = np.array(train_X_vec)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_X_vec = np.array(val_X_vec)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "test_X_vec = np.array(test_X_vec)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_SEQ_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.0005), metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_val = ModelCheckpoint('models/{epoch:02d}_val_{val_loss:.4f}.h5', monitor='val_acc', verbose=1, save_best_only=True,mode=\"min\")\n",
    "\n",
    "history = model.fit(train_X_vec, \n",
    "                    train_y, \n",
    "                    validation_data=(val_X_vec, val_y), \n",
    "                    callbacks=[checkpoint_val],\n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(x=test_X_vec, y=test_y)\n",
    "\n",
    "Y_test = np.argmax(test_y, axis=1)\n",
    "y_pred = model.predict_classes(test_X_vec)\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('categorical cross-entropy loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tdt4310)",
   "language": "python",
   "name": "tdt4310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
